{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c895794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T12:32:16.971530Z",
     "start_time": "2023-05-28T12:32:13.853049Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2d96ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T12:32:28.232018Z",
     "start_time": "2023-05-28T12:32:27.875983Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from pathlib import Path\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41a697c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T12:32:29.238515Z",
     "start_time": "2023-05-28T12:32:28.915940Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fourz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fourz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e60eacb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T12:39:59.843803Z",
     "start_time": "2023-05-28T12:39:36.773619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime in city \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "series_title                                         The Outsider\n",
       "release_year                                                 2020\n",
       "runtime                                                      60.0\n",
       "genre                                       Crime, Drama, Fantasy\n",
       "rating                                                        7.7\n",
       "cast            Ben Mendelsohn, Bill Camp, Jeremy Bobb, Mare W...\n",
       "synopsis        Investigators are confounded over an unspeakab...\n",
       "end_year                                                   2020.0\n",
       "Name: 517, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ModelWord2Vec:\n",
    "    \"\"\"\n",
    "    Класс для обучения и использования модели Word2Vec.\n",
    "\n",
    "    Аргументы:\n",
    "        final_df (pandas.DataFrame): pandas DataFrame, содержащий данные для обучения модели.\n",
    "\n",
    "    Атрибуты:\n",
    "        vectors (list): Список векторов, представляющих слова в DataFrame final_df.\n",
    "        model (gensim.models.Word2Vec): Обученная модель Word2Vec.\n",
    "\n",
    "    Методы:\n",
    "        create_model(self): Обучает модель Word2Vec.\n",
    "        vec_df(self): Преобразует DataFrame final_df в DataFrame, содержащий векторы для каждого слова.\n",
    "    \"\"\"\n",
    "    def __init__(self, final_df: pd.DataFrame):\n",
    "        self.vectors = []\n",
    "        self.final_df = final_df\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Обучает модель Word2Vec.\n",
    "\n",
    "        Возвращает:\n",
    "            gensim.models.Word2Vec: Обученная модель Word2Vec.\n",
    "        \"\"\"\n",
    "        self.model = Word2Vec(\n",
    "            self.final_df[\"synopsis_tokens\"],\n",
    "            window=6,\n",
    "            vector_size=50,\n",
    "            alpha=0.03,\n",
    "            min_alpha=0.0007,\n",
    "            negative=20,\n",
    "            workers=cores - 1,\n",
    "            sg=0\n",
    "        )\n",
    "        self.model.train(\n",
    "            self.final_df[\"synopsis_tokens\"],\n",
    "            total_examples=len(self.final_df[\"synopsis_tokens\"]),\n",
    "            epochs=1\n",
    "        )\n",
    "        return self.model\n",
    "\n",
    "    def vec_df(self):\n",
    "        \"\"\"\n",
    "        Преобразует DataFrame final_df в DataFrame, содержащий векторы для каждого слова.\n",
    "\n",
    "        Возвращает:\n",
    "            pandas.DataFrame: DataFrame, содержащий векторы для каждого слова.\n",
    "        \"\"\"\n",
    "        for synopsis_tokens in self.final_df[\"synopsis_tokens\"]:\n",
    "            synopsis_vector = np.zeros(self.model.vector_size)\n",
    "            num_tokens = 0\n",
    "            for token in synopsis_tokens:\n",
    "                if token in self.model.wv.key_to_index:\n",
    "                    vector = self.model.wv.get_vector(token)\n",
    "                    synopsis_vector += vector\n",
    "                    num_tokens += 1\n",
    "            if num_tokens > 0:\n",
    "                synopsis_vector /= num_tokens\n",
    "                self.vectors.append(synopsis_vector)\n",
    "        self.final_df[\"vectors\"] = self.vectors\n",
    "        \n",
    "        return self.final_df\n",
    "\n",
    "    def predict(self, synopsis: str):\n",
    "        \"\"\"\n",
    "        Предсказывает название серии для данного синопсиса.\n",
    "\n",
    "        Аргументы:\n",
    "            synopsis: Синопсис серии.\n",
    "\n",
    "        Возвращает:\n",
    "            Предсказанное название серии.\n",
    "        \"\"\"\n",
    "        synopsis = preprocessor_text(synopsis) \n",
    "\n",
    "        lst = set([i for i in synopsis.split() if i in self.model.wv.key_to_index])\n",
    "        \n",
    "        # Получение векторов слов в синопсисе.\n",
    "        context_word_vectors = [self.model.wv[word] for word in lst]\n",
    "\n",
    "        # Расчет среднего вектора слов для синопсиса.\n",
    "        predicted_vector = np.mean(context_word_vectors, axis=0)\n",
    "\n",
    "        # Расчет сходства между предсказанным вектором и векторами в наших данных\n",
    "        similarities = df['vectors'].map(lambda x: 1 - spatial.distance.cosine(np.array(x, dtype='float16'), predicted_vector.astype('float16')))\n",
    "\n",
    "        # сортируем топ самую малую разницу\n",
    "        most_similar_index = similarities.argmax()\n",
    "        \n",
    "        return df.iloc[similarities.argmax()]\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Аргументы:\n",
    "        df: Фрейм данных.\n",
    "\n",
    "    Возвращает:\n",
    "        df + токенизированные колонки.\n",
    "    \"\"\"\n",
    "    df.dropna(subset=['release_year', 'rating', 'cast'], inplace=True)\n",
    "    df[['release_year']] = df[['release_year']].astype(int)\n",
    "    df[\"synopsis_tokens\"] = df[\"synopsis1\"].apply(word_tokenize)\n",
    "#     df['series_title'] = df['series_title'].drop_duplicates().dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocessor_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Нормализует текстовую строку.\n",
    "\n",
    "    Аргументы:\n",
    "        text: Текстовая строка для нормализации.\n",
    "\n",
    "    Возвращает:\n",
    "        Нормализованную текстовую строку.\n",
    "    \"\"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # удаление стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\n",
    "        \"C:/Users/fourz/OneDrive/Рабочий стол/Word2Vec/filter_data.csv\",\n",
    "        usecols=lambda column: column != 'Unnamed: 0' and column != 'synopsis_tokens'\n",
    "    )\n",
    "    synaps = input()\n",
    "\n",
    "    final_df = preprocess(df)\n",
    "\n",
    "    pp = preprocessor_text(synaps)\n",
    "\n",
    "    model_word2vec = ModelWord2Vec(final_df)\n",
    "    model = model_word2vec.create_model()\n",
    "\n",
    "    emb = model_word2vec.vec_df()\n",
    "    \n",
    "    pred = model_word2vec.predict(pp)\n",
    "    \n",
    "    display(pred[['series_title', 'release_year', 'runtime', 'genre', 'rating', 'cast', 'synopsis', 'end_year']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9770903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T00:07:51.121269Z",
     "start_time": "2023-05-27T00:07:51.090887Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dabf53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
